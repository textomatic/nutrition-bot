{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from haystack.utils import launch_es\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import TextConverter, PreProcessor\n",
    "from haystack.nodes import BM25Retriever\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.utils import print_answers\n",
    "from haystack.nodes import PDFToTextConverter\n",
    "from pathlib import Path\n",
    "from haystack import Document\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)\n",
    "\n",
    "from helper_functions import download_all_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store_research = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"document_research\",\n",
    "    similarity=\"dot_product\",\n",
    "    embedding_dim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233558"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document_store_research.delete_all_documents()\n",
    "len(document_store_research.get_all_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize constants\n",
    "pdf_dir_local = \"../data/all_pdfs/\"\n",
    "cloud_storage_bucket_name = \"aipi540_nlp_nutrition\"\n",
    "cloud_storage_folder = \"all_pdfs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all research papers from google cloud storage\n",
    "download_all_pdfs(cloud_storage_bucket_name, cloud_storage_folder, pdf_dir_local)\n",
    "\n",
    "pdfs_to_index = [pdf_dir_local + \"/\" + f for f in os.listdir(pdf_dir_local)]\n",
    "\n",
    "all_pdfs_df = pd.DataFrame(columns=[\"content\", \"doi\"])\n",
    "\n",
    "pdf_text_converter = PDFToTextConverter(\n",
    "    remove_numeric_tables=True,\n",
    "    valid_languages=[\"en\"]\n",
    ")\n",
    "\n",
    "for pdf in tqdm(pdfs_to_index):\n",
    "    meta = {\"doi\": pdf.replace(pdf_dir_local+\"/\", \"\").replace(\".pdf\", \"\").replace(\"_\", \"/\")}\n",
    "    doc = pdf_text_converter.convert(file_path=Path(pdf), meta=meta)\n",
    "    rp_df = pd.DataFrame({\"content\": doc[0].to_dict()[\"content\"], \"doi\": doc[0].to_dict()[\"meta\"][\"doi\"]}, index=[0])\n",
    "    all_pdfs_df = pd.concat([all_pdfs_df, rp_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_pipeline = [hero.preprocessing.remove_whitespace,\n",
    "                   hero.preprocessing.remove_angle_brackets,\n",
    "                   hero.preprocessing.remove_html_tags,\n",
    "                   hero.preprocessing.remove_urls,\n",
    "                   hero.preprocessing.remove_square_brackets]\n",
    "\n",
    "all_pdfs_df['content_clean'] = hero.clean(all_pdfs_df['content'], custom_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for content, doi in zip(list(all_pdfs_df[\"content_clean\"]), list(all_pdfs_df[\"doi\"])):\n",
    "    doc = Document(content, meta={\"doi\": doi})\n",
    "    data_list.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=150,\n",
    "    split_overlap=10,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b599e5c0d64fe4a7be1b6452725d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/2481 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document c1a26373e276e41b0e45e1a320ee4850 is 11313 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document eb12156bacb92eea559c5c49fc03cf8f is 11178 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 821ee66f1b8b1903ee04e622dadc7c4c is 10990 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document c61f16edeffbcb0328bfef42175a6eb7 is 11185 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 317cce305a2f1b3061976a6c37197685 is 12883 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 3f2ed28bcf61f4cc1810e7d83266e819 is 15279 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 571261c2ae8b62fa83b558c5742969ad is 13004 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 48d20f51d698dede2a4054fe1c7ba196 is 11347 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 1d00300d75a407e748dc8f705addc4fc is 21872 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n"
     ]
    }
   ],
   "source": [
    "preprocessed_docs = preprocessor.process(documents=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "document_store_research.write_documents(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
      "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store_research,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43df9a558bfc4a509495b1bcb6e2af4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating embeddings:   0%|          | 0/63558 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131b2e6f2d994154ad4a07589a232403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4f139df6054369ababe1b896e3f532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5768b20737d948b087ce12e223680c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bcbe03da9a47f495d9186a012494f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9036c75a14748ceafed9639aaae8665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f5270827d4612927a20d1478a9ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a004fa9ee8e44804b5fd10c6f1b39189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/3568 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_store_research.update_embeddings(retriever, update_existing_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"Query\"])\n",
    "indexing_pipeline.add_node(component=document_store_research, name=\"DocumentStore\", inputs=[\"PreProcessor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cda71e9e62a4ee1801167ec5379b026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/2481 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.nodes.preprocessor.preprocessor:We found one or more sentences whose word count is higher than the split length.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document c1a26373e276e41b0e45e1a320ee4850 is 11313 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document eb12156bacb92eea559c5c49fc03cf8f is 11178 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 821ee66f1b8b1903ee04e622dadc7c4c is 10990 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document c61f16edeffbcb0328bfef42175a6eb7 is 11185 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 317cce305a2f1b3061976a6c37197685 is 12883 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 3f2ed28bcf61f4cc1810e7d83266e819 is 15279 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 571261c2ae8b62fa83b558c5742969ad is 13004 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 48d20f51d698dede2a4054fe1c7ba196 is 11347 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 1d00300d75a407e748dc8f705addc4fc is 21872 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m indexing_pipeline\u001b[39m.\u001b[39;49mrun_batch(documents\u001b[39m=\u001b[39;49mdata_list)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/pipelines/base.py:687\u001b[0m, in \u001b[0;36mPipeline.run_batch\u001b[0;34m(self, queries, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRunning node \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m` with input: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, node_id, node_input)\n\u001b[0;32m--> 687\u001b[0m     node_output, stream_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49mnodes[node_id][\u001b[39m\"\u001b[39;49m\u001b[39mcomponent\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49m_dispatch_run_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnode_input)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# The input might be a really large object with thousands of embeddings.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39m# If you really want to see it, raise the log level.\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mException while running node \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with input \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, node_id, node_input)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/nodes/base.py:206\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run_batch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_dispatch_run_batch\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    The Pipelines call this method when run_batch() is executed. This method in turn executes the\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    _dispatch_run_general() method with the correct run method.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch_run_general(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/nodes/base.py:243\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run_general\u001b[0;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m run_signature_args:\n\u001b[1;32m    241\u001b[0m         run_inputs[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m--> 243\u001b[0m output, stream \u001b[39m=\u001b[39m run_method(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrun_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrun_params)\n\u001b[1;32m    245\u001b[0m \u001b[39m# Collect debug information\u001b[39;00m\n\u001b[1;32m    246\u001b[0m debug_info \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/document_stores/base.py:581\u001b[0m, in \u001b[0;36mBaseDocumentStore.run_batch\u001b[0;34m(self, documents, index, headers, id_hash_keys)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_batch\u001b[39m(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    576\u001b[0m     documents: List[Union[\u001b[39mdict\u001b[39m, Document]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m     id_hash_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m ):\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(documents\u001b[39m=\u001b[39;49mdocuments, index\u001b[39m=\u001b[39;49mindex, headers\u001b[39m=\u001b[39;49mheaders, id_hash_keys\u001b[39m=\u001b[39;49mid_hash_keys)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/document_stores/base.py:571\u001b[0m, in \u001b[0;36mBaseDocumentStore.run\u001b[0;34m(self, documents, index, headers, id_hash_keys)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    570\u001b[0m         doc_objects\u001b[39m.\u001b[39mappend(d)\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_documents(documents\u001b[39m=\u001b[39;49mdoc_objects, index\u001b[39m=\u001b[39;49mindex, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    572\u001b[0m \u001b[39mreturn\u001b[39;00m {}, \u001b[39m\"\u001b[39m\u001b[39moutput_1\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/document_stores/search_engine.py:450\u001b[0m, in \u001b[0;36mSearchEngineDocumentStore.write_documents\u001b[0;34m(self, documents, index, batch_size, duplicate_documents, headers)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Pass batch_size number of documents to bulk\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(documents_to_index) \u001b[39m%\u001b[39m batch_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 450\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bulk(documents_to_index, request_timeout\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, refresh\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh_type, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    451\u001b[0m         documents_to_index \u001b[39m=\u001b[39m []\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m documents_to_index:\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/document_stores/search_engine.py:210\u001b[0m, in \u001b[0;36mSearchEngineDocumentStore._bulk\u001b[0;34m(self, documents, headers, request_timeout, refresh, _timeout, _remaining_tries)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39mBulk index documents using a custom retry logic with\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39mexponential backoff and exponential batch size reduction to avoid overloading the cluster.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m:param _remaining_tries: Number of remaining retries\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_bulk(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient, documents, request_timeout\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, refresh\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh_type, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mstatus_code\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m429\u001b[39m:  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/haystack/document_stores/elasticsearch.py:177\u001b[0m, in \u001b[0;36mElasticsearchDocumentStore._do_bulk\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_bulk\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    176\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Override the base class method to use the Elasticsearch client\"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m bulk(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/helpers/actions.py:410\u001b[0m, in \u001b[0;36mbulk\u001b[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[1;32m    409\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39myield_ok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[39mfor\u001b[39;00m ok, item \u001b[39min\u001b[39;00m streaming_bulk(\n\u001b[1;32m    411\u001b[0m     client, actions, ignore_status\u001b[39m=\u001b[39mignore_status, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    412\u001b[0m ):\n\u001b[1;32m    413\u001b[0m     \u001b[39m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ok:\n\u001b[1;32m    415\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stats_only:\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/helpers/actions.py:329\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mmin\u001b[39m(max_backoff, initial_backoff \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (attempt \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[1;32m    328\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mfor\u001b[39;00m data, (ok, info) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    330\u001b[0m         bulk_data,\n\u001b[1;32m    331\u001b[0m         _process_bulk_chunk(\n\u001b[1;32m    332\u001b[0m             client,\n\u001b[1;32m    333\u001b[0m             bulk_actions,\n\u001b[1;32m    334\u001b[0m             bulk_data,\n\u001b[1;32m    335\u001b[0m             raise_on_exception,\n\u001b[1;32m    336\u001b[0m             raise_on_error,\n\u001b[1;32m    337\u001b[0m             ignore_status,\n\u001b[1;32m    338\u001b[0m             \u001b[39m*\u001b[39margs,\n\u001b[1;32m    339\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    340\u001b[0m         ),\n\u001b[1;32m    341\u001b[0m     ):\n\u001b[1;32m    343\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ok:\n\u001b[1;32m    344\u001b[0m             action, info \u001b[39m=\u001b[39m info\u001b[39m.\u001b[39mpopitem()\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/helpers/actions.py:240\u001b[0m, in \u001b[0;36m_process_bulk_chunk\u001b[0;34m(client, bulk_actions, bulk_data, raise_on_exception, raise_on_error, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     ignore_status \u001b[39m=\u001b[39m (ignore_status,)\n\u001b[1;32m    238\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[39m# send the actual request\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     resp \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mbulk(\u001b[39m*\u001b[39;49margs, body\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(bulk_actions) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m TransportError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     gen \u001b[39m=\u001b[39m _process_bulk_chunk_error(\n\u001b[1;32m    243\u001b[0m         error\u001b[39m=\u001b[39me,\n\u001b[1;32m    244\u001b[0m         bulk_data\u001b[39m=\u001b[39mbulk_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m         raise_on_error\u001b[39m=\u001b[39mraise_on_error,\n\u001b[1;32m    248\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/client/utils.py:347\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    346\u001b[0m         params[p] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(p)\n\u001b[0;32m--> 347\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/client/__init__.py:472\u001b[0m, in \u001b[0;36mElasticsearch.bulk\u001b[0;34m(self, body, index, doc_type, params, headers)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEmpty value passed for a required argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    471\u001b[0m body \u001b[39m=\u001b[39m _bulk_body(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransport\u001b[39m.\u001b[39mserializer, body)\n\u001b[0;32m--> 472\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransport\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    473\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    474\u001b[0m     _make_path(index, doc_type, \u001b[39m\"\u001b[39;49m\u001b[39m_bulk\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    475\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    476\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    477\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    478\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/transport.py:427\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    424\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_connection()\n\u001b[1;32m    426\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     status, headers_response, data \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    428\u001b[0m         method,\n\u001b[1;32m    429\u001b[0m         url,\n\u001b[1;32m    430\u001b[0m         params,\n\u001b[1;32m    431\u001b[0m         body,\n\u001b[1;32m    432\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    433\u001b[0m         ignore\u001b[39m=\u001b[39;49mignore,\n\u001b[1;32m    434\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     \u001b[39m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     headers_response \u001b[39m=\u001b[39m {\n\u001b[1;32m    439\u001b[0m         header\u001b[39m.\u001b[39mlower(): value \u001b[39mfor\u001b[39;00m header, value \u001b[39min\u001b[39;00m headers_response\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    440\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/elasticsearch/connection/http_urllib3.py:255\u001b[0m, in \u001b[0;36mUrllib3HttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    252\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gzip_compress(body)\n\u001b[1;32m    253\u001b[0m     request_headers[\u001b[39m\"\u001b[39m\u001b[39mcontent-encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 255\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    256\u001b[0m     method, url, body, retries\u001b[39m=\u001b[39;49mRetry(\u001b[39mFalse\u001b[39;49;00m), headers\u001b[39m=\u001b[39;49mrequest_headers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    258\u001b[0m response_headers \u001b[39m=\u001b[39m {\n\u001b[1;32m    259\u001b[0m     header\u001b[39m.\u001b[39mlower(): value \u001b[39mfor\u001b[39;00m header, value \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    260\u001b[0m }\n\u001b[1;32m    261\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1349\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    317\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/haystack/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indexing_pipeline.run_batch(documents=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
      "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store_research,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:haystack.document_stores.search_engine:Updating embeddings for all 361981 docs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0572dd35f31647e097b7339309071d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating embeddings:   0%|          | 0/361981 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f630b966dc548f8a2503d6b89645221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_store_research.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
      "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
      "INFO:haystack.modeling.model.language_model: * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)\n",
      "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n",
      "INFO:haystack.modeling.model.language_model:Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.\n",
      "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "querying_pipeline = Pipeline()\n",
    "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2e5b297df54756aa24ca7440f2f2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = querying_pipeline.run(\n",
    "    query=\"Is sourdough bread good for health?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 10},\n",
    "        \"Reader\": {\"top_k\": 5}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [<Answer {'answer': 'sourdough bread is healthy', 'type': 'extractive', 'score': 0.44866815209388733, 'context': 'used by sourdough fermentation and a\\nconsumer perception that sourdough bread is healthy(33),\\nstudies evaluating the role of sourdough for appetite an', 'offsets_in_document': [{'start': 594, 'end': 620}], 'offsets_in_context': [{'start': 62, 'end': 88}], 'document_ids': ['647c6f56157c5c829d2dead43aae7b4e'], 'meta': {'_split_id': 13, 'year': 1999}}>,\n",
      "             <Answer {'answer': 'sourdough bread induced a significantly\\nlower plasma glucose response', 'type': 'extractive', 'score': 0.2772848308086395, 'context': '0, 60, 120, and 180 min.\\nIn IGT subjects sourdough bread induced a significantly\\nlower plasma glucose response at 30 minutes (p = 0.048)\\nand a smaller', 'offsets_in_document': [{'start': 399, 'end': 468}], 'offsets_in_context': [{'start': 41, 'end': 110}], 'document_ids': ['9e356b16035b2dc5bd46f8c710fe96f5'], 'meta': {'_split_id': 5, 'year': 1999}}>,\n",
      "             <Answer {'answer': 'leavened for 8 h using a\\nstarter', 'type': 'extractive', 'score': 0.26184171438217163, 'context': 'Sourdough bread was leavened for 8 h using a\\nstarter containing autochthonous Saccharomyces cerevisiae\\nand several bacilli able to produce a significa', 'offsets_in_document': [{'start': 20, 'end': 52}], 'offsets_in_context': [{'start': 20, 'end': 52}], 'document_ids': ['a7b83d2fe92faa200c1c92e83e27b5f8'], 'meta': {'_split_id': 4, 'year': 1999}}>,\n",
      "             <Answer {'answer': 'improve its structure and flavour', 'type': 'extractive', 'score': 0.2594502866268158, 'context': 'ntation is commonly used in the production\\nof rye bread to improve its structure and flavour(20). Beneficial\\nAbbreviation: GLP-1, glucagon-like peptid', 'offsets_in_document': [{'start': 74, 'end': 107}], 'offsets_in_context': [{'start': 59, 'end': 92}], 'document_ids': ['512c79129a8063cc13702970393539e0'], 'meta': {'_split_id': 10, 'year': 1999}}>,\n",
      "             <Answer {'answer': 'improve its structure and flavour', 'type': 'extractive', 'score': 0.24877825379371643, 'context': 'ntation is commonly used in the production\\nof rye bread to improve its structure and flavour(20). Beneficial\\nAbbreviation: GLP-1, glucagon-like peptid', 'offsets_in_document': [{'start': 512, 'end': 545}], 'offsets_in_context': [{'start': 59, 'end': 92}], 'document_ids': ['1884c03c3b7b61687951640a996046c9'], 'meta': {'_split_id': 9, 'year': 1999}}>],\n",
      " 'documents': [<Document: {'content': 'Sourdough bread was leavened for 8 h using a\\nstarter containing autochthonous Saccharomyces cerevisiae\\nand several bacilli able to produce a significant amount of\\nD-and L-lactic acid, whereas the reference bread was\\nleavened for 2 h with commercial baker yeast containing\\nSaccharomyces cerevisiae. Plasma glucose and insulin\\nlevels were measured at time 0, 30, 60, 120, and 180 min.\\n', 'content_type': 'text', 'score': 0.6972861707173587, 'meta': {'_split_id': 4, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a7b83d2fe92faa200c1c92e83e27b5f8'}>,\n",
      "               <Document: {'content': 'Sourdough bread was leavened for 8 h using a\\nstarter containing autochthonous Saccharomyces cerevisiae\\nand several bacilli able to produce a significant amount of\\nD-and L-lactic acid, whereas the reference bread was\\nleavened for 2 h with commercial baker yeast containing\\nSaccharomyces cerevisiae. Plasma glucose and insulin\\nlevels were measured at time 0, 30, 60, 120, and 180 min.\\nIn IGT subjects sourdough bread induced a significantly\\nlower plasma glucose response at 30 minutes (p = 0.048)\\nand a smaller incremental area under curve (AUC) D 030\\nand D 060 min (p = 0.020 and 0.018 respectively) in\\ncomparison to the bread leavened with baker yeast. ', 'content_type': 'text', 'score': 0.6870087943389153, 'meta': {'_split_id': 5, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e356b16035b2dc5bd46f8c710fe96f5'}>,\n",
      "               <Document: {'content': 'The sourdough, with an in-house culture mixture, was\\nFermented crispbread and postprandial responses 687\\x0cfermented for 40 h and then mixed with the other ingredients.\\nThis was followed by a two-step fermentation, first for 120 min\\nat 29C, followed by 35 min with an increase from 30 to 38C.\\nUnfermented rye crispbread was mixed with water at 12C and\\nthen whipped at 6C to incorporate air into the dough.\\n', 'content_type': 'text', 'score': 0.6861211215295777, 'meta': {'_split_id': 28, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1dc45e86e12a2e7646e64f3f334c7fb9'}>,\n",
      "               <Document: {'content': 'The sourdough is a mixture of water and flour fer-\\nmented by lactic acid bacteria and yeasts, responsible\\nfor acidification and leavening, respectively. The\\nbackslopping technique  which consists in keeping\\nthe sourdough active by using the sourdough of the\\nprevious fermentation cycle as a starter to ferment a\\nnew mixture of flour and water (Ercolini et al. 2013)\\nallows to select a stable and characteristic microbiota\\nable to give to bakery products some specific sensor-\\nial, structural and nutritional characteristics. ', 'content_type': 'text', 'score': 0.6842347494104867, 'meta': {'_split_id': 8, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2cc2e9a9625562758c6aaef7de7cd9dd'}>,\n",
      "               <Document: {'content': 'Sourdough fermentation is commonly used in the production\\nof rye bread to improve its structure and flavour(20). Beneficial\\nAbbreviation: GLP-1, glucagon-like peptide-1.\\n* Corresponding author: G. Zamaratskaia, fax +46 18 672995, email galia.zamaratskaia@slu.se\\n These authors contributed equally to this work.\\n Present address: Institute of Biological Chemistry and Nutrition, University of Hohenheim, Garbenstrasse 28, Stuttgart 70599, Germany.\\n Present address: Department of Biology and Biological Engineering, Chalmers University of Technology, Gothenburg, Sweden.\\n The Authors 2017\\x0ceffects in the form of reduced insulin and glucose responses\\nhave also been demonstrated for sourdough breads(21,22). ', 'content_type': 'text', 'score': 0.6837192258601629, 'meta': {'_split_id': 10, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '512c79129a8063cc13702970393539e0'}>,\n",
      "               <Document: {'content': \"The whole grain wheat sourdough bread con-\\nsisted predominantly of whole grain wheat flour (37% dry\\nweight) and also contained other ingredients including\\nnon-wheat grains (18% dry weight: whole grain spelt and\\nrye flours, brown flax seeds, rolled oats, cracked soy, yel-\\nlow flax seeds, millet seeds, malted barley flour, brown\\nrice flour, millet flour, durum semolina and organic sour-\\ndough made from natural bacterial culture, whole grain\\nspelt and rye flours).\\nThe quantity of treatment breads provided was deter-\\nmined to correspond with 65% of the daily grain serving\\nrecommendation of Canada's Food Guide (CFG) [38],\\nwhich, based on a bread slice of 35 g, equated to 136.5-\\n159.3-182.0 g breadd-1 (7-8 grain servings/d) for men.\\n\", 'content_type': 'text', 'score': 0.683635748504376, 'meta': {'_split_id': 31, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b5fe75a51a07a27cac7f9a31a8828e02'}>,\n",
      "               <Document: {'content': 'Keywords Sourdough bread \\nImpaired glucose tolerance  Lactic acid\\nIntroduction\\nUntil a few years ago in several areas of south Europe the\\nMediterranean diet was widely diffused and characterised\\nby the prevailing consumption of low-calorie food, satu-\\nrated fats, rich in fibers and carbohydrates with low\\nglycaemic index. In particular bread represented the main\\ncomponent of the daily diet in such countries including\\nSardinia [1]. In the traditional agropastoral society of this\\nisland bread represented the base of nutrition and until\\napproximately 30 years ago it was made nearly in all the\\nvillages of the island using homemade microbic starters,\\nwhich were exchanged among families. ', 'content_type': 'text', 'score': 0.6818743558984744, 'meta': {'_split_id': 10, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c92b6ab464073cc5b653ccbbd7dbf1c8'}>,\n",
      "               <Document: {'content': 'The mechanisms underlying the beneficial\\neffects of rye foods have not been fully elucidated, although spe-\\ncific features of rye bread microstructure, such as a dense structure\\nand an amylose layer surrounding starch granules, have been\\nsuggested to be important factors(19). However, processing can alter\\nmicrostructure, composition and bioavailability of different com-\\npounds, which might affect the postprandial responses to a food.\\nSourdough fermentation is commonly used in the production\\nof rye bread to improve its structure and flavour(20). Beneficial\\nAbbreviation: GLP-1, glucagon-like peptide-1.\\n', 'content_type': 'text', 'score': 0.6817884487117766, 'meta': {'_split_id': 9, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1884c03c3b7b61687951640a996046c9'}>,\n",
      "               <Document: {'content': 'Moreover, soluble viscous dietary fibres\\n(arabinoxylan and -glucan) are degraded during fermentation\\nby endogenous enzymes, leading to reduced molecular\\nweight(31). This has been suggested as a mechanism explaining\\na lower insulin response to unfermented compared with yeast-\\nfermented rye crispbread(16), because the high molecular\\nweight of soluble fibre contributes to increased digesta\\nviscosity, which might reduce gastric emptying rate and the\\ndigestion and absorption rate of nutrients(32). Despite demon-\\nstrated effects caused by sourdough fermentation and a\\nconsumer perception that sourdough bread is healthy(33),\\nstudies evaluating the role of sourdough for appetite and\\nmetabolic responses remain scarce.\\n', 'content_type': 'text', 'score': 0.6792174832472085, 'meta': {'_split_id': 13, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '647c6f56157c5c829d2dead43aae7b4e'}>,\n",
      "               <Document: {'content': 'Recent\\nstudies demonstrated that sourdough fermentation\\nincreases the amount of free phenolic compounds [40],\\nwhich have also been reported to have an impact on\\nlowering the glycemic and insulin responses [41,42].\\nHowever, it is not likely that the higher phenolic acids\\ncontent of the RB than in WB can explain the observed\\ndecreased insulin response after RB. Most phenolic\\nacids in cereals are ester linked to polymers and require\\nseveral biotransformations until they can exert their\\nbeneficial effects in the body. ', 'content_type': 'text', 'score': 0.6792078964034896, 'meta': {'_split_id': 90, 'year': 1999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3a1f12b3fe924604bd2449bcef29835'}>],\n",
      " 'no_ans_gap': 7.516536235809326,\n",
      " 'node_id': 'Reader',\n",
      " 'params': {'Reader': {'top_k': 5}, 'Retriever': {'top_k': 10}},\n",
      " 'query': 'Is sourdough bread good for health?',\n",
      " 'root_node': 'Query'}\n"
     ]
    }
   ],
   "source": [
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Is sourdough bread good for health?\n",
      "Answers:\n",
      "[   {   'answer': 'sourdough bread is healthy',\n",
      "        'context': 'used by sourdough fermentation and a\\n'\n",
      "                   'consumer perception that sourdough bread is healthy(33),\\n'\n",
      "                   'studies evaluating the role of sourdough for appetite an',\n",
      "        'score': 0.44866815209388733},\n",
      "    {   'answer': 'sourdough bread induced a significantly\\n'\n",
      "                  'lower plasma glucose response',\n",
      "        'context': '0, 60, 120, and 180 min.\\n'\n",
      "                   'In IGT subjects sourdough bread induced a significantly\\n'\n",
      "                   'lower plasma glucose response at 30 minutes (p = 0.048)\\n'\n",
      "                   'and a smaller',\n",
      "        'score': 0.2772848308086395},\n",
      "    {   'answer': 'leavened for 8 h using a\\nstarter',\n",
      "        'context': 'Sourdough bread was leavened for 8 h using a\\n'\n",
      "                   'starter containing autochthonous Saccharomyces cerevisiae\\n'\n",
      "                   'and several bacilli able to produce a significa',\n",
      "        'score': 0.26184171438217163},\n",
      "    {   'answer': 'improve its structure and flavour',\n",
      "        'context': 'ntation is commonly used in the production\\n'\n",
      "                   'of rye bread to improve its structure and flavour(20). '\n",
      "                   'Beneficial\\n'\n",
      "                   'Abbreviation: GLP-1, glucagon-like peptid',\n",
      "        'score': 0.2594502866268158},\n",
      "    {   'answer': 'improve its structure and flavour',\n",
      "        'context': 'ntation is commonly used in the production\\n'\n",
      "                   'of rye bread to improve its structure and flavour(20). '\n",
      "                   'Beneficial\\n'\n",
      "                   'Abbreviation: GLP-1, glucagon-like peptid',\n",
      "        'score': 0.24877825379371643}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"medium\" ## Choose from `minimum`, `medium` and `all`\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
